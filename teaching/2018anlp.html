<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
   <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <meta name="Author" content="Tatjana Scheffler">
   <title>Advanced NLP, winter semester 2018/2019</title>
   <meta name="description" content="Tatjana Scheffler - Twitterseminar">
   <link href="../tshp.css" rel="stylesheet" type="text/css" />

</head>
<body>

<div  id="container">
<h1>Advanced Natural Language Processing</h1>

<p>
  <a href="#schedule">Schedule</a> 
</p>

<h2>Course organization</h2>

<p>
<table cellspacing="10">
  <TR>
    <TD width="100">Instructor:</TD>
    <TD><a href="http://www.ling.uni-potsdam.de/~scheffler/">Tatjana
      Scheffler</a></TD>
  </TR>
  <TR valign="top">
    <TD>TA:</TD>
    <TD>Edit Sz端gyi</TD>
  </TR>
  <TR valign="top">
    <TD>Time:</TD>
    <TD>Tuesdays & Fridays, 10 a.m.-12 </TD>
  </TR>
   <TR>
    <TD>Place:</TD>
    <TD>Golm, building 14, room 009 </TD>
  </TR>
   <TR>
    <TD>Modules:</TD>
    <TD>CSBM1 (M.Sc. Cognitive Systems)</TD>
  </TR>
   <TR>
    <TD>Moodle:</TD>
    <TD>Please register on the course's Moodle site.</TD>
  </TR>
</table>
</p>

<h2>Requirements</h2>
<ul>
<li>regular readings</li>
<li>active participation</li>
<li>completion of the assignments: At least 5 (out of 6) assignments
must be turned in. 
I will not accept late assignments. </li>
</ul>
<h2>Grading policy</h2>

<h3>Passing the course</h3>
<p>To be admitted to the module exam, you need to pass the course. For
this, we will grade the best two assignments out of each half of the
semester (i.e., the best 2 from the first 3 + the best 2 from the
second 3). At least 250 points in total (out of 400) in these 4
assignments are needed to pass the course. 
</p>

<h3>Module grade</h3>
<p>
The grade will be based on a collaborative <b>final project</b>, to be completed during the
semester break. There are four graded deliverables for this
project:</p>
<p>
<ol>
  <li>a planning paper (individual)</li>
  <li>a project presentation (group)</li>
  <li>the implemented project (group)</li>
  <li>a project report (individual)</li>
</ol>
</p>
<p>
The grade will be composed equally from these four parts. Details will
be discussed in class.
</p>

<h2>Course description</h2>

<p>
This class is the graduate-level introduction to computational
linguistics, a first-year class in the MSc Cognitive Systems. The
purpose of this class is to introduce the important concepts, models
and methods used in natural language processing (NLP). After the successful
completion of this course, students should be able to (i) read and
understand the scientific literature in the area of computational
linguistics and (ii) start implementing their own NLP projects.
</p>
<p>
We will cover the following topics:
</p>
<ul>
  <li>statistical models of language</li>
  <li>part of speech tagging (HMMs)</li>
  <li>syntactic parsing (PCFGs, others?)</li>
  <li>semantics</li>
  <li>machine translation</li>
  <li>speech processing</li>
  <li>classification</li>
  <li>and more</li>
</ul>

<h2><a id="schedule" >Schedule</a></h2>

<p>Readings in J/M are marked for the <a
href="https://web.stanford.edu/~jurafsky/slp3/" >third edition</a>
unless marked otherwise.
</p>

</div>
<div id="bigtable">
<table frame="void" rules="none" width="100%">
<tbody>
<tr>
<th>Date</td>
<th>Topic</td>
<th>Readings</td>
<th>Assignments</td>
</tr>
<tr>
<td>T 10/16</td>
<td><a href="" >Introduction</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>F 10/19</td>
<td>Review: Probability Theory</td>
<td>Harald Goldstein, <a
  href="http://folk.uio.no/nhfehr/Goldstein%20-%20Introduction%20to%20Probability%20Theory.pdf"
  >A short introduction to probability and related concepts</a><br />
And/Or: Manning/Sch端tze, chapter 2.1<br />
Optional:  Kevin Murphy, <a href="http://www.cs.ubc.ca/~murphyk/Teaching/CS340-Fall06/reading/bernoulli.pdf" >Binomial and multinomial distributions</a></td>
<td></td>
</tr>
<tr>
<td>T 10/23</td>
<td>N-grams</td>
<td>Jurafsky/Martin, chapters 3.1-3.2 <br /><a href="https://www.youtube.com/watch?v=fCn8zs912OE" >A video about Zipf's law</a></td>
<td>A1 released</td>
</tr>
<tr>
<td>F 10/26</td>
<td>Smoothing language models</td>
<td>Jurafsky/Martin, chapters 3.4-3.7<br />
  Opt.: Manning/Sch端tze, chapters 6.2-6.3 <br />
 <a href="https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1">Chen/Goodman, 1998</a></td>
<td></td>
</tr>
<tr>
<td>T 30/10</td>
<td>Classification</td>
<td>Jurafsky/Martin, chapter 4</td>
<td></td>
</tr>
<tr>
<td>F 11/2</td>
<td>Classification (2)</td>
<td>Jurafsky/Martin, chapter 5 </td>
<td>A1 due, A2 released</td>
</tr>
<tr>
<td>T 11/6</td>
<td>Part of speech tagging</td>
<td>Jurafsky/Martin <b>(2nd ed.!)</b>, chapters 5.1-5.5, 6.1-6.5; J/M (3rd ed.) ch. 8.1-8.4 </td>
<td></td>
</tr>
<tr>
<td>F 11/9</td>
<td><i>discussion of A1</i> (Edit)</td>
<td><span style="color: #2288aa;">be prepared to present your solutions!</span></td>
<td></td>
</tr>
<tr>
<td>T 11/13</td>
<td>HMM Training</td>
<td>Jurafsky/Martin, rest of chapter 8 (-8.4) + Appendix A<br />
<a href="http://www.cs.jhu.edu/~jason/465/PowerPoint/lect24-hmm.xls"
  >HMM spreadsheet</a>, Eisner's ice cream example<br />
<a href="http://www.cs.jhu.edu/~jason/papers/eisner.tnlp02.pdf" >Eisner's paper explaining how to work with the spreadsheet</a></td>
<td></td>
</tr>
<tr>
<td>F 11/16</td>
<td>Context free grammars, CKY parsing</td>
<td>Jurafsky/Martin, chapter 11 (-11.2), <br /> J/M chapter 10 as
  background<br />
  further reading: Santorini/Kroch, <a href="http://www.ling.upenn.edu/~beatrice/syntax-textbook/" >Online syntax textbook</a><br />
<a href="http://lxmls.it.pt/2015/cky.html">CKY animation</a></td>
<td>A2 due, A3 released</td>
</tr>
<tr>
<td>T 11/20</td>
<td>PCFGs</td>
<td>Jurafsky/Martin, chapters 12.1-12.5, 12.7</td>
<td></td>
</tr>
<tr>
<td>F 11/23</td>
<td><i>discussion of A2</i></td>
<td><span style="color: #2288aa;">be prepared to present your solutions!</span></td>
<td></td>
</tr>
<tr>
<td>T 11/27</td>
<td><span style="color: #ff0000;"><i>dies academicus (no class)</i></span></td>
<td></td>
<td></td>
</tr>
<tr>
<td>F 11/30</td>
<td>Training PCFGs</td>
<td>Jurafsky/Martin, chapter 12<br />
Manning/Sch端tze, chapter 11<br />
Michael Collins, <a href="http://www.cs.columbia.edu/~mcollins/io.pdf" >The inside-outside algorithm.</a> </td>
<td>A3 due, A4 released</td>
</tr>
<tr>
<td>T 12/4</td>
<td>Advanced PCFG models</td>
<td>Mark Johnson (1998), <a
  href="http://www.aclweb.org/anthology/J/J98/J98-4004.pdf" >PCFG
  Models of Linguistic Tree Representations </a>(esp. on parent
  annotations)<br />
Michael Collins, <a
  href="http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/lexpcfgs.pdf"
  >Lexicalized PCFGs</a><br />
Dan Klein/Chris Manning (2003), <a href="http://www.aclweb.org/anthology/P/P03/P03-1054.pdf" >Accurate unlexicalized parsing</a></td>
<td></td>
</tr>
<tr>
<td>F 12/7</td>
<td><i>discussion of A3</i></td>
<td><span style="color: #2288aa;">be prepared to present your
  solutions!</span></td>
<td></td>
</tr>
<tr>
<td>T 12/11</td>
<td>Dependency parsing</td>
<td>McDonald/Pereira/Ribarov/Hajic (2005),
  <a href="http://www.aclweb.org/anthology/H05-1066" >Non-projective
  Dependency Parsing using Spanning Tree Algorithms</a><br />
Joakim Nivre (2008), <a href="http://www.aclweb.org/anthology/J08-4003" >Algorithms for Deterministic Incremental Dependency Parsing</a></td>
<td></td>
</tr>
<tr>
<td>F 12/14</td>
<td>Statistical machine translation: Alignments</td>
<td>Jurafsky/Martin (2nd ed.), ch. 25 (through 25.6)<br />
Adam Lopez, <a
  href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.421.5497&rep=rep1&type=pdf"
  >Word Alignment and the Expectation-Maximization Algorithm</a>
  tutorial (try <a href="https://pdfs.semanticscholar.org/8338/914856defebe908394c2b33dc43d350c5dd0.pdf?_ga=1.153336056.1178462967.1483354457" >here</a>) <br />
  <a href="http://mt-class.org/" >http://mt-class.org/</a><br />
  <a href="http://mttalks.ufal.ms.mff.cuni.cz/index.php?title=Main_Page" >MT Talks</a></td>
<td>A4 due</td>
</tr>
<tr>
<td>T 12/18</td>
<td>Phrase-based machine translation</td>
<td>Jurafsky/Martin (2nd ed.), ch. 25</td>
<td></td>
</tr>
<tr>
<td><span style="color: #ff0000;">T 12/18, 12:30pm</span></td>
<td><i>discussion of A3</i></td>
<td><span style="color: #2288aa;">be prepared to present your
  solutions!</span></td>
<td></td>
</tr>
<tr>
<td>
12/24-30</p>
</td>
<td><span style="color: #ff0000;">no class (winter break)</span></td>
<td></td>
<td></td>
</tr>
<tr>
<td>
12/31-1/6</p>
</td>
<td><span style="color: #ff0000;">no class (winter break)</span></td>
<td></td>
<td></td>
</tr>
<tr>
<td>T 1/8</td>
<td>Syntax-based machine translation</td>
<td>David Chiang, <a href="http://www.mitpressjournals.org/doi/pdf/10.1162/coli.2007.33.2.201" >Hierarchical phrase-based translation</a>. Computational Linguistics, 2007.</td>
<td>A5 distributed</td>
</tr>
<tr>
<td>F 1/11</td>
<td>Semantic parsing</td>
<td>Zettlemoyer/Collins, <a href="http://www.cs.columbia.edu/~mcollins/papers/uai05.pdf" >Learning to Map Sentences to
  Logical Form. Structured Classification with Probabilistic
  Categorial Grammars</a>. 2005 <br />
Wong/Mooney, <a href="http://www.cs.utexas.edu/~ml/papers/wasp-naacl-06.pdf" >Learning for Semantic Parsing with
  Statistical Machine Translation</a>. HLT-NAACL, 2006 <br />
Opt.: Mark Steedman, <a href="http://www.inf.ed.ac.uk/teaching/courses/nlg/readings/ccgintro.pdf" >A very short introduction to CCG</a>. 1996 </td>
<td></td>
</tr>
<tr>
<td>T 1/15</td>
<td>Lexical semantics</td>
<td>Jurafsky/Martin, ch. 6 (+ Appendix C) <br />
Further reading:  Mitchell/Lapata (2008), <a href="http://anthology.aclweb.org/P/P08/P08-1028.pdf" >Vector-based models of
  semantic composition</a>; 
Baroni/Zamparelli (2010), <a href="http://www.aclweb.org/anthology/D/D10/D10-1115.pdf" >Nouns are vectors, adjectives are matrices</a></td>
<td></td>
</tr>
<tr>
<td>F 1/18</td>
<td>Speech recognition + synthesis</td>
<td>Jurafsky/Martin (2nd ed.), ch. 8,9<br />
Links for further reading/system samples: <br />
<a href="https://arxiv.org/abs/1610.05256" >"Human parity" speech recognition</a><br />
<a href="http://languagelog.ldc.upenn.edu/nll/?p=28894" >Language log on human parity speech recognition</a><br />
<a href="https://www.google.com/intl/en/chrome/demos/speech.html" >Google speech API web demo</a><br />
<a href="http://sebastian.germes.in/blog/2011/09/googles-speech-api/" >S. Germesin on using the speech API remotely</a><br />
<a href="http://mary.dfki.de" >MARY TTS (DFKI)</a><br />
<a
  href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/"
  >WaveNet: Generating raw audio</a><br />
<a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41539.pdf">Deep Learning in Speech Synthesis</a><br />
<a href="http://emosamples.syntheticspeech.de/" >Expressive and emotional synthetic speech</a></td>
<td>A5 due, A6 released</td>
</tr>
<tr>
<td>T 1/22</td>
<td>LDA</td>
<td>David Blei, <a href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf" >Probabilistic topic models</a>. 2012<br />
Links for further reading: <br />
Steyvers/Griffiths, <a
  href="http://psiexp.ss.uci.edu/research/papers/SteyversGriffithsLSABookFormatted.pdf"
  >Probabilistic Topic Models</a>. 2007.<br />
Darling, <a href="http://u.cs.biu.ac.il/~89-680/darling-lda.pdf" >A Theoretical and Practical Implementation
Tutorial on Topic Modeling and Gibbs Sampling</a>.<br />
<a href="https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d" >Online Topic Modelling Tutorial</a> and <a href="https://lettier.com/projects/lda-topic-modeling/" >Demo for LDA</a></td>
<td></td>
</tr>
<tr>
<td>F 1/25</td>
<td><i>discussion of A5</i></td>
<td><span style="color: #2288aa;">be prepared to present your
  solutions!</span></td>
<td></td>
</tr>
<tr>
<td>T 1/29</td>
<td>Intro to deep neural networks</td>
<td>Richard Socher,<a href="http://nlp.stanford.edu/~socherr/thesis.pdf" > Recursive Deep Learning for Natural Language
  Processing and Computer Vision</a> (Chapter 2). 2014<br />
Further reading (for intuitions): <a
  href="http://karpathy.github.io/neuralnets/"
  >http://karpathy.github.io/neuralnets/</a>, <a href="http://neuralnetworksanddeeplearning.com/chap4.html" >http://neuralnetworksanddeeplearning.com/chap4.html</a></td>
<td></td>
</tr>
<tr>
<td>F 2/1</td>
<td><i>presentations of final projects, final discussion</i></td>
<td><span style="color: #2288aa;">all</span></td>
<td>A6 due</td>
</tr>
<tr>
<td>2/4-10</td>
<td><span style="color: #ff0000;">no class</span></td>
<td></td>
<td></td>
</tr>


</tbody>
</table>
</div>
<div id="container">

<h2>Literature</h2>

<ul>
  <li>Dan Jurafsky and James Martin, <i>Speech and Language
  Processing</i>. (2008 or 2013 edition)</li>
  <li>Chris Manning and Hinrich Sch端tze, <i>Foundations of Statistical Natural Language Processing</i>. </li>
</ul>
<p>
Most computational linguists own both of these books. We will assign weekly
readings, so you should ensure you get your own copy or have access to
the copies that are available in the university library.
</p>

<ul>
  <li><i>Natural Language Processing with Python</i>. (the NLTK book)
  Available <a href="http://www.nltk.org/book/" >here</a>.</li>
</ul>


</div>

<div class="foot">
<!-- hhmts start -->Last modified: Tue Jan 22 10:02:16 CET 2019 <!-- hhmts end -->
</div>
</body>
</html>
